---
title: "Your Document Title"
author: "Document Author"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: GitHub
---

H20 Es un paquete creado por la compañía [H2O.ai](https://www.h2o.ai/) que busca combinar los principales algoritmos de *machine learning* y aprendizaje estadístico con el *Big Data*. Su forma de comprimir y almacenar los datos permite trabajar con millones de registros (empleando todos sus cores) o un *cluster* de muchos computadores.

```{r results='hide', message=FALSE}
# Import libraries
library(mosaicData)
```

# Función para imprimir datatable

```{r results='hide', message=FALSE}
library(tidyverse)
library(kableExtra)
# Tablas páginadas customizadas
library(DT)
# Funcion para obtener datatable (datos, indice tabla, nombre)
custom_dt <- function(data, indice = "n", nombre = "Tabla") {
  table_n <- paste("Table ", indice) 
  dt <- datatable(data,
                  caption = htmltools::tags$caption(
                  style = 'caption-side: bottom; text-align: center;',
                  table_n, htmltools::em(nombre)),
                  extensions = 'FixedColumns',
                  rownames= FALSE,
                  filter = 'top',
                  options = list(
                  pageLength = 5,
                  autoWidth = TRUE,
                  columnDefs = 
                  list(list(width = '300px', targets = c(0,7))),
                  scrollX = TRUE,
                  escape = T)
                  )
  return(dt)
}
```

```{r}
data("SaratogaHouses", package = "mosaicData")
data <- SaratogaHouses
head(data)
```

```{r}
colnames(data)
```

```{r}
kable(summary(data))
```

# Paquetería de visualización

```{r}
library(ggpubr) 
```

# Conexión a h2o

```{r results='hide', message=FALSE}
# Carga de h2o
library(h2o)
```

## Inicialización de H2o

```{r}
h2o.init(ip = 'localhost',
         nthreads = -1, # -1 indica que se emplean todos los cores disponibles
         max_mem_size = "6g" # Máxima memoria disponible para el cluster
         )
```

```{r}
h2o.removeAll() # Borrar lo que tenía en memoria antes el cluster
# h2o.no_progress()   # Para que no se muestre la barra de progreso.
```

## Cargar los datos en ambiente h20

```{r}
data_h2o <- as.h2o(x = data, destination_frame = "data_h2o")
```

## Cargar los datos en un CSV

```{r}
write.csv(data,"saratogaHouses.csv")
```

# Funciones simples ambiente h20

Con dim, se evaluan las dimensiones del set de datos, en este caso se tienen 1728 observaciones con 16 columnas.

```{r}
# Dimensiones del set de datos
obj = h2o.dim(data_h2o)
typeof(obj)
col_count = obj[1]
row_count =  obj[2]
col_count
```

Es posible revisar los nombres de las columnas de la siguiente forma.

```{r}
col_names = h2o.colnames(data_h2o)
col_names[1] # 0 said type, index starts in 1
```

La función `h2o.describe()` es similar a la función `summary()`, es muy útil para obtener un análisis rápido que muestre el tipo de datos, la cantidad de valores ausentes, el valor mínimo, máximo, media, desviación típica y el número de categorías (Cardinality) de cada una de las variables. `h2o` emplea el nombre `enum` para los datos de tipo `factor` o `character`.

En este caso podemos observar que no hay valores faltantes, sin embargo, hay varias columnas con zeros.

```{r}
custom_dt(h2o.describe(data_h2o), 1, "h2o describe table")
```

## Mostrar los indices de las columnas

Donde coltype:

A character string indicating which column type to filter by. This must be one of the following: "numeric" - Numeric, but not categorical or time "categorical" - Integer, with a categorical/factor String mapping "string" - String column "time" - Long msec since the Unix Epoch - with a variety of display/parse options "uuid" - UUID "bad" - No none-NA rows (triple negative! all NAs or zero rows)

```{r}
indices_col <- h2o.columns_by_type(data_h2o, coltype = "categorical")
indices_col
```

## Nombre de las columnas de indices_col

Puedes filtrar un set de datos con más de un indice, por ejemplo"

```{r}
h2o.colnames(data_h2o)[indices_col]
```

## Correlación de Pearson

Es posible definir el coeficiente de correlación de Pearson como un índice que puede utilizarse para medir el grado de relación de dos variables siempre y cuando ambas sean cuantitativas y continuas.

```{r}
# Obtener el índice de las columnas numéricas
indices_col <- h2o.columns_by_type(data_h2o, coltype = "numeric")
# [,indices_col] todas las files, solo las columnas numéricas
numeric_cols = data_h2o[,indices_col]
head(numeric_cols)
```

### Data frame correlación de pearson

```{r}
# Al indicar 'y' como NULL, se mide la relación todas contra todas.
# na.rm indica que los valores nulos deben ser removidos
h2o_cor_df <- h2o.cor(x = numeric_cols, y = NULL, method = "Pearson", na.rm = TRUE)
h2o_cor_df
```

## Tabla con el número de observaciones

```{r}
tabla_conteo_fuel <- as.data.frame(h2o.table(data_h2o$fuel))
# h2o.table Uses the cross-classifying factors to build a table of counts at each combination of factor levels
# Usando as.data.frame lo convierto al ambiente de r
tabla_conteo_fuel
```

## Visualización en ambiente de R (ggplot)

```{r}
tabla_conteo_fuel %>% ggplot(aes(x=fuel, y=Count, fill = fuel)) + geom_col()
```

Podemos hacer que se vea un poco mejor el gráfico:

```{r}
ggplot(
  data = tabla_conteo_fuel,
  aes(x = fuel, y = Count, fill = fuel)) +
geom_col() +
scale_fill_manual(values = c("gray50", "orangered2","blue")) +
theme_bw() +
labs(
  x = "Fuel", y = "Numero de observaciones",
  title = "Distribucion de la variable Fuel") +
theme(legend.position = "none")
```

## Modelos en H20

Objetivo: predecir el precio de las casas

### Split de los datos. Entrenamiento y validación

H2o utiliza para este proposito SplitFrame después de generar las particiones

```{r}
# Conjuntos de train y test
# 0.8 se toma para la primera particion y con el resto hace lo demás
particiones     <- h2o.splitFrame(data = data_h2o, ratios = c(0.8), seed = 666)
particiones
# Es posible observar que tenemos dos tablas accesibles por [[1]] y [[2]]
```

```{r}
datos_train_h2o   <- h2o.assign(data = particiones[[1]], key = "datos_test_H2O")
datos_test_h2o  <- h2o.assign(data = particiones[[2]], key = "datos_test_H2O")
datos_train_h2o
```

En el momento en que consideremos la validación, debemos agregar en los ratios el porcentaje de la validación, en este caso será train (60%), validación (20%) y test (20%). En la semilla se le agrega el numeral 4 y se adiciona un nuevo subconjunto de datos, entendiendo que el 1 es train, el 2 es validación y el 3 es test.

```{r}
particiones_h2o <- h2o.splitFrame(data = data_h2o, ratios = c(0.6,0.2), seed = 123)
```

```{r}
# Conjuntos de train, validación y test
data_train_h2o <- h2o.assign(data = particiones_h2o[[1]], key = "data_train_h2o")
data_validation_h2o <- h2o.assign(data = particiones_h2o[[2]], key = "data_validation_h2o")
data_test_h2o <- h2o.assign(data = particiones_h2o[[3]], key = "data_test_h2o")
```

Corremos un summary de los tres conjuntos y nos fijamos que sus distribuciones sean similares:

```{r}
summary(data_train_h2o$price)
```

```{r}
summary(data_validation_h2o$price)
```

```{r}
summary(data_test_h2o$price)
```

## Procesamiento de los datos

H2o Automatiza algunas etapas del procesamiento de datos. Por lo que el preprocesado es intrinseco al entrenamiento. En concreto: - Variables categóricas en h2o : h20 identifica las variables categóricas y crea internamente las variables dummy. - Estandarización: Estandariza los predictores numéricos para que todos tengan media cero y varianza uno. - Para correr lasso, ridge, Deep Learning o algunos otros modelos es necesario realizar la estandarización de predictores - Eliminación de variables con varianza 0 - No se deben incluir en un modelo predictores con valor unico (varianza 0) - Balance de clases

## Modelo con Random Forest

En esta ocasión haremos uso de los datos de validación, sin embargo; pueden omitirse.

```{r}
random_forest_model <- h2o.randomForest(
  training_frame = data_train_h2o, # datos de h2o para training
  validation_frame = data_validation_h2o, # datos de h2o para validación (no es requerido)
  x = 2:16, # Las columnas predictoras, por índice
  y = 1,    # La columna que queremos predecir, variable objetivo
  model_id = "rf_covType_v1",  # nombre del modelo en h2o
  ntrees = 200, # número de árboles
  stopping_rounds = 2, # Para cuando el promedio de dos árboles está dentro de 0.001 (predeterminado)
  score_each_iteration = T, # Predecir contra training y validación para cada árbol
  seed = 666  # Establecer una semilla aleatoria para que se pueda reproducir
)
```

### Detalles del modelo en H2o

Al correr el summary del modelo podemos ver que - Se tienen 17 árboles, profundidad máxima de 20 y maximo 689 hojas. - Se observa que livingArea es la variable más importante con un 26%

```{r}
summary(random_forest_model)
```

## Modelo con Gradient Boosting Machine

Este algorítmo tradicionalmente super random forest. Es usado típicamente para tareas de regresión y clasificación. Brinda además un modelo de predicción en la forma de un ensamble de otros modelos de predicción como los árboles de decisión.

En H2o podemos observar una estructura muy similar a la del random forest, ahora utilizaremos la función h2o.gbm y lo que cambiaremos para este caso, es el model_id. NOTA: En la mayoría de los algoritmos el primero es para regresión y el segundo para clasificación.

```{r}
gbm_model <- h2o.gbm(
  training_frame = data_train_h2o, # datos de h2o para training
  validation_frame = data_validation_h2o, # datos de h2o para validación (no es requerido)
  x = 2:16, # Las columnas predictoras, por índice
  y = 1,    # La columna que queremos predecir, variable objetivo
  model_id = "gbm_covType1", # nombre del modelo en h2o (regresión)
  seed = 2000000   # Establecer una semilla aleatoria para que se pueda reproducir
)
```

### Detalles del modelo GBM

```{r}
summary(gbm_model)
```

Podemos ver la evolución del modelo, para evaluar cómo aprende el modelo a medida que se añaden nuevos árboles al ensamble.

h2o almacena las métricas de entrenamiento y test bajo el nombre de scoring. Los valores se encuentran almacenados dentro del modelo. En los modelos GBM, se puede estudiar la influencia de los predictores cuantificando la reducción total de error cuadrático que ha conseguido cada predictor en el conjunto de todos los árboles que forman el modelo. Por tanto, cuando aumenta la cantidad de árboles, disminuye el RMSE.

```{r}
scoring <- as.data.frame(gbm_model@model$scoring_history)
head(scoring)
```

La importacia se mantienme similar aunque podemos observar que, por ejemplo; livingArea ahora representa el 53%. Esto sucedió al reducir el error cuadrático medio.

```{r}
importancia <- as.data.frame(gbm_model@model$variable_importances)
importancia
```

### Grafiquemos la importancia de estos predictores en la GBM. Donde 'scaled_importance' es una de las columnas:

```{r}
library(ggplot2)
ggplot(data = importancia,
       aes(x = reorder(variable, scaled_importance), y = scaled_importance)) +
  geom_col() +
  coord_flip() +
  labs(title = "Importancia de los predictores en el modelo GBM",
       subtitle = "Importancia en base a la reduccion del error cuadratico medio",
       x = "Predictor",
       y = "Importancia relativa") +
  theme_bw()
```

## Modelo GBM cambiando parámetros

### Comparación entre GBM y Random Forest

```{r}
gbm_model@model$validation_metrics
```

### Summary Métricas modelos

MSE: RMSE: MAE: RMSLE: Mean Residual Deviance :
